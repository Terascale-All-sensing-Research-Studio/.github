At the [Terascale All-sensing Research Studio (TARS)](https://tars.clarkson.edu/) at Clarkson University, we perform research in human-driven artificial intelligence using capture and analysis of dense multi-person interactions in online and real-world environments. Our studio houses two multi-modal sensing spaces---the Gazebo, a dense multi-modal sensing space consisting of 16 Kinect sensors, 192 226-FPS color cameras, 16 thermal cameras, 16 sEMG sensors, 16 AMD Ryzen machines each with a 1080 Ti GPU, and 24 Intel 6-core machines to study multi-person interactions, and the Cube, a space with 4 Kinects, 5 thermal cameras, and 4 226-FPS color cameras to study two-person interactions. It also houses two Kinova Gen3 robotic manipulators, one LoCoBot mobile robot manipulator, multiple lab-built drones, a lab-built walker robot, and several VR systems, 3D printers, 3D scanners, a compute cluster with 275,000+ CUDA cores and 4,800+ Tensor cores spread over 50+ GPUs, and a 1 petabyte data center. The core of our work is in understanding how humans perform tasks in the real-world, to inform next generation artificial intelligence (AI), robotics, and virtual reality. Our research spans computer vision, deep learning, computer graphics, and human-computer interaction. The studio is funded through multiple NSF grants.
